{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 可变形卷积偏移量梯度形状演示 ===\n",
      "\n",
      "输入特征图: [2, 64, 32, 32]\n",
      "输出特征图: [2, 128, 32, 32]\n",
      "卷积核大小: 3×3, 采样点数量: 9\n",
      "\n",
      "=== 前向传播过程中的形状变化 ===\n",
      "采样后特征形状: (2, 64, 9, 32, 32)\n",
      "调制后特征形状: (2, 64, 9, 32, 32)\n",
      "卷积输出形状: (2, 128, 32, 32)\n",
      "\n",
      "=== 反向传播过程中的形状变化 ===\n",
      "对调制特征的梯度形状: torch.Size([2, 64, 9, 32, 32])\n",
      "对采样特征的梯度形状: torch.Size([2, 64, 9, 32, 32])\n",
      "几何敏感度形状: (2, 64, 9, 32, 32, 2)\n",
      "最终偏移量梯度形状: torch.Size([2, 18, 32, 32])\n",
      "\n",
      "=== 形状变化总结 ===\n",
      "                输入特征: torch.Size([2, 64, 32, 32])\n",
      "                偏移量场: torch.Size([2, 18, 32, 32])\n",
      "               调制因子场: torch.Size([2, 9, 32, 32])\n",
      "                上层梯度: torch.Size([2, 128, 32, 32])\n",
      "             对调制特征梯度: torch.Size([2, 64, 9, 32, 32])\n",
      "               几何敏感度: (2, 64, 9, 32, 32, 2)\n",
      "               偏移量梯度: torch.Size([2, 18, 32, 32])\n",
      "\n",
      "==================================================\n",
      "矩阵操作详细演示\n",
      "==================================================\n",
      "模拟参数: N=2, C_out=32, C_in=16, K=3\n",
      "输出空间尺寸: 8×8\n",
      "\n",
      "原始权重形状: torch.Size([32, 16, 3, 3])\n",
      "flatten(1)后形状: torch.Size([32, 144])\n",
      "转置后形状: torch.Size([144, 32])\n",
      "上层梯度展平后形状: torch.Size([2, 32, 64])\n",
      "扩展后权重形状: torch.Size([2, 144, 32])\n",
      "矩阵乘法结果形状: torch.Size([2, 144, 64])\n",
      "最终columns形状: torch.Size([2, 16, 3, 3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def demonstrate_offset_grad_shapes():\n",
    "    print(\"=== 可变形卷积偏移量梯度形状演示 ===\\n\")\n",
    "    \n",
    "    # 设置基本参数\n",
    "    batch_size = 2\n",
    "    in_channels = 64\n",
    "    out_channels = 128\n",
    "    height, width = 32, 32\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    \n",
    "    # 计算输出尺寸\n",
    "    H_out = (height + 2 * padding - kernel_size) // stride + 1\n",
    "    W_out = (width + 2 * padding - kernel_size) // stride + 1\n",
    "    num_points = kernel_size * kernel_size\n",
    "    \n",
    "    print(f\"输入特征图: [{batch_size}, {in_channels}, {height}, {width}]\")\n",
    "    print(f\"输出特征图: [{batch_size}, {out_channels}, {H_out}, {W_out}]\")\n",
    "    print(f\"卷积核大小: {kernel_size}×{kernel_size}, 采样点数量: {num_points}\")\n",
    "    print()\n",
    "    \n",
    "    # 模拟可变形卷积的各个组件\n",
    "    # 1. 输入特征图\n",
    "    x = torch.randn(batch_size, in_channels, height, width, requires_grad=True)\n",
    "    \n",
    "    # 2. 偏移量场 (由额外的卷积层生成)\n",
    "    offset_field = torch.randn(batch_size, 2 * num_points, H_out, W_out, requires_grad=True)\n",
    "    \n",
    "    # 3. 调制因子场 (Deformable Conv v2)\n",
    "    modulation_field = torch.sigmoid(torch.randn(batch_size, num_points, H_out, W_out, requires_grad=True))\n",
    "    \n",
    "    # 4. 主卷积权重\n",
    "    weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size, requires_grad=True)\n",
    "    \n",
    "    # 5. 模拟上层梯度 (∂L/∂y)\n",
    "    grad_output = torch.randn(batch_size, out_channels, H_out, W_out)\n",
    "    \n",
    "    print(\"=== 前向传播过程中的形状变化 ===\")\n",
    "    \n",
    "    # 模拟双线性插值采样过程\n",
    "    # 在实际实现中，这里会有复杂的坐标计算和插值\n",
    "    # 这里我们简化为一个占位张量来表示采样后的特征\n",
    "    sampled_features_shape = (batch_size, in_channels, num_points, H_out, W_out)\n",
    "    print(f\"采样后特征形状: {sampled_features_shape}\")\n",
    "    \n",
    "    # 调制后的特征\n",
    "    modulation_expanded = modulation_field.unsqueeze(1)  # [N, 1, K², H_out, W_out]\n",
    "    modulated_features_shape = (batch_size, in_channels, num_points, H_out, W_out)\n",
    "    print(f\"调制后特征形状: {modulated_features_shape}\")\n",
    "    \n",
    "    # 卷积输出 (简化的前向传播)\n",
    "    # 在实际中，这里会涉及复杂的采样和加权求和\n",
    "    output_shape = (batch_size, out_channels, H_out, W_out)\n",
    "    print(f\"卷积输出形状: {output_shape}\")\n",
    "    \n",
    "    print(\"\\n=== 反向传播过程中的形状变化 ===\")\n",
    "    \n",
    "    # 模拟偏移量梯度计算的关键步骤\n",
    "    \n",
    "    # 步骤1: 计算对调制后特征的梯度 (∂L/∂x_modulated)\n",
    "    # 这对应于之前讨论的 columns = W^T × grad_output\n",
    "    \n",
    "    # 重塑权重和梯度以进行矩阵乘法\n",
    "    weight_flat = weight.view(out_channels, in_channels * num_points)  # [C_out, C_in*K²]\n",
    "    weight_T = weight_flat.transpose(0, 1)  # [C_in*K², C_out]\n",
    "    \n",
    "    grad_output_flat = grad_output.view(batch_size, out_channels, H_out * W_out)  # [N, C_out, H_out*W_out]\n",
    "    \n",
    "    # 计算梯度: ∂L/∂x_modulated = W^T × (∂L/∂y)\n",
    "    grad_modulated_features = torch.bmm(\n",
    "        weight_T.unsqueeze(0).expand(batch_size, -1, -1),  # [N, C_in*K², C_out]\n",
    "        grad_output_flat  # [N, C_out, H_out*W_out]\n",
    "    )  # 结果: [N, C_in*K², H_out*W_out]\n",
    "    \n",
    "    grad_modulated_features = grad_modulated_features.view(\n",
    "        batch_size, in_channels, num_points, H_out, W_out\n",
    "    )\n",
    "    \n",
    "    print(f\"对调制特征的梯度形状: {grad_modulated_features.shape}\")\n",
    "    \n",
    "    # 步骤2: 计算对原始采样特征的梯度 (考虑调制因子的影响)\n",
    "    # ∂L/∂x_sampled = ∂L/∂x_modulated ⊙ (1/modulation)\n",
    "    # 这里简化处理，实际需要考虑调制因子的导数\n",
    "    grad_sampled_features = grad_modulated_features * modulation_expanded\n",
    "    \n",
    "    print(f\"对采样特征的梯度形状: {grad_sampled_features.shape}\")\n",
    "    \n",
    "    # 步骤3: 模拟几何敏感度计算 (这是偏移量梯度的核心)\n",
    "    # 在实际实现中，这里会计算双线性插值核相对于坐标的导数\n",
    "    \n",
    "    # 模拟几何敏感度张量\n",
    "    # 对于每个采样点，我们需要x和y两个方向的敏感度\n",
    "    geometric_sensitivity_shape = (batch_size, in_channels, num_points, H_out, W_out, 2)\n",
    "    geometric_sensitivity = torch.randn(geometric_sensitivity_shape)\n",
    "    print(f\"几何敏感度形状: {geometric_sensitivity_shape}\")\n",
    "    \n",
    "    # 步骤4: 计算偏移量梯度\n",
    "    # ∂L/∂Δp = sum_over_channels( ∂L/∂x_sampled × 几何敏感度 )\n",
    "    \n",
    "    # 扩展梯度维度以进行点积\n",
    "    grad_sampled_expanded = grad_sampled_features.unsqueeze(-1)  # [N, C, K², H_out, W_out, 1]\n",
    "    \n",
    "    # 计算点积并求和通道维度\n",
    "    offset_grad = torch.sum(grad_sampled_expanded * geometric_sensitivity, dim=1)  # [N, K², H_out, W_out, 2]\n",
    "    \n",
    "    # 重塑为标准的偏移量场形状 [N, 2*K², H_out, W_out]\n",
    "    offset_grad = offset_grad.permute(0, 1, 4, 2, 3).contiguous()  # [N, K², 2, H_out, W_out]\n",
    "    offset_grad = offset_grad.view(batch_size, 2 * num_points, H_out, W_out)\n",
    "    \n",
    "    print(f\"最终偏移量梯度形状: {offset_grad.shape}\")\n",
    "    \n",
    "    print(\"\\n=== 形状变化总结 ===\")\n",
    "    shapes = {\n",
    "        \"输入特征\": x.shape,\n",
    "        \"偏移量场\": offset_field.shape,\n",
    "        \"调制因子场\": modulation_field.shape,\n",
    "        \"上层梯度\": grad_output.shape,\n",
    "        \"对调制特征梯度\": grad_modulated_features.shape,\n",
    "        \"几何敏感度\": geometric_sensitivity_shape,\n",
    "        \"偏移量梯度\": offset_grad.shape\n",
    "    }\n",
    "    \n",
    "    for name, shape in shapes.items():\n",
    "        print(f\"{name:>20}: {shape}\")\n",
    "    \n",
    "    return offset_grad\n",
    "\n",
    "def demonstrate_matrix_operations():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"矩阵操作详细演示\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 模拟具体的矩阵操作\n",
    "    N, C_out, C_in, K = 2, 32, 16, 3\n",
    "    H_out, W_out = 8, 8\n",
    "    num_points = K * K\n",
    "    \n",
    "    print(f\"模拟参数: N={N}, C_out={C_out}, C_in={C_in}, K={K}\")\n",
    "    print(f\"输出空间尺寸: {H_out}×{W_out}\")\n",
    "    print()\n",
    "    \n",
    "    # 权重矩阵\n",
    "    weight = torch.randn(C_out, C_in, K, K)\n",
    "    print(f\"原始权重形状: {weight.shape}\")\n",
    "    \n",
    "    # flatten(1) 操作\n",
    "    weight_flat = weight.flatten(1)\n",
    "    print(f\"flatten(1)后形状: {weight_flat.shape}\")\n",
    "    \n",
    "    # transpose(0, 1) 操作\n",
    "    weight_T = weight_flat.transpose(0, 1)\n",
    "    print(f\"转置后形状: {weight_T.shape}\")\n",
    "    \n",
    "    # 上层梯度\n",
    "    grad_output = torch.randn(N, C_out, H_out, W_out)\n",
    "    grad_output_flat = grad_output.view(N, C_out, H_out * W_out)\n",
    "    print(f\"上层梯度展平后形状: {grad_output_flat.shape}\")\n",
    "    \n",
    "    # 矩阵乘法: columns = weight_T × grad_output_flat\n",
    "    # 需要扩展weight_T以匹配batch维度\n",
    "    weight_T_expanded = weight_T.unsqueeze(0).expand(N, -1, -1)\n",
    "    print(f\"扩展后权重形状: {weight_T_expanded.shape}\")\n",
    "    \n",
    "    columns = torch.bmm(weight_T_expanded, grad_output_flat)\n",
    "    print(f\"矩阵乘法结果形状: {columns.shape}\")\n",
    "    \n",
    "    # 重塑回合适的形状\n",
    "    columns_reshaped = columns.view(N, C_in, K, K, H_out, W_out)\n",
    "    print(f\"最终columns形状: {columns_reshaped.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    offset_grad = demonstrate_offset_grad_shapes()\n",
    "    demonstrate_matrix_operations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
